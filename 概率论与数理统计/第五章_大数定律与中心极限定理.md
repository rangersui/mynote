# 大数定律
## 参考文献
[概率论——大数定律与中心极限定理 - tetradecane - 知乎](https://zhuanlan.zhihu.com/p/259280292)

[大数定律与中心极限定理 - Xurtle - CSDN](https://blog.csdn.net/xlinsist/article/details/79486734)

《概率论与数理统计》浙江大学

## 弱大数定理(辛钦大数定理)
假设$X_1,X_2,\dots,X_n,\dots$是独立同分布(independent and identically distributed)随机变量,数学期望为$\mu$,方差(variance)为$\sigma^{2}$.对于每一个n,设$\bar{X}_n$为前n个变量的平均值.然后对于任意$\epsilon\gt0$,有
$\lim_{n\to\infty}{P(\vert\bar{X}_n-\mu\vert\lt\epsilon)}=1$.

认为$\epsilon$是均值与数学期望$\mu$的微小误差。辛钦大数定律从理论上指出：用算术平均值来近似实际真值是合理的。而在数理统计中，这一定律使得用算术平均值来估计数学期望有了理论依据。

## 马尔可夫不等式(Markov inequalities)
> 马尔可夫不等式给出了随机变量的函数大于等于某正数的概率的上界。

如果$X$是一个非负随机变量且$t\gt0$，则$X$大于$t$的概率小于等于$X$的数学期望除以$t$: $P(X\ge t)\le\frac{E[X]}{t}$

### 证明
因为$X\gt0$,$E(X)=\int_{0}^{\infty}xp(x)dx=\int_{0}^{t}xp(x)dx+\int_{t}^{\infty}xp(x)dx\ge\int_{t}^{\infty}xp(x)dx\ge t\int_{t}^{\infty}p(x)dx=tP(X\gt t)$

可得$P(X\gt t)\le \frac{E(X)}{t}$

## 切比雪夫不等式(Chebyshev inequalities)
> 切比雪夫不等式可以使人们在随机变量X的分布未知的情况下，对事件概率作出估计。

设$X$(可积,integrable)为随机变量有数学期望值$\mu$和非零方差$\sigma^2$:$P(\vert X-\mu\vert\ge c)\le\frac{\sigma^2}{c^2}$

令$c=k\sigma$,对于任何实数$k\gt0$,上述公式变为$P(\vert X-\mu\vert\ge k\sigma)\le\frac{1}{k^2}$

可以写成如下形式:$P\{\vert X-\mu\vert\lt\epsilon\}\gt1-\frac{\sigma^2}{\epsilon^2}$

### 证明
使用马尔可夫不等式,令$t=c^2$则
$P((X-\mu)^2\ge c^2)\le\frac{E[(X-\mu)^2]}{c^2}=\frac{\sigma^2}{c^2}$

当$c=k\sigma$,则$P(\vert X-\mu\vert\ge k\sigma)\le\frac{\sigma^2}{k^2\sigma^2}=\frac{1}{k^2}$

## 依概率收敛(Convergence in probability)
> 一个随机变量序列$X_n(n\ge1)$ 依概率收敛到某一个随机变量$a$，指的是$X_n$和$a$之间存在一定差距的可能性将会随着$n$的增大而趋向于零。依概率收敛是测度论中的依测度收敛概念在概率论中的特例。

对于任意$\epsilon$满足$\lim_{n\to+\infty}P(\vert X_n-a\vert\ge\epsilon)=0$

## 弱大数定理证明
$X_1,X_2,\dots$独立同分布，并且在数学期望存在的情况下

因为: $E(\frac{1}{n}\sum_{k=1}^nX_k)=\frac{1}{n}E(\sum_{k=1}^nE(X_k)=\frac{1}{n}n\mu=\mu$

又由独立性可得: $D(\frac{1}{n}\sum_{k=1}^{n}X_k)=\frac{1}{n}D(\sum_{k=1}^{n}X_k)=\frac{1}{n^2}n\sigma^2=\frac{\sigma^2}{n}$

再由切比雪夫不等式可得: $1\ge P\{\vert\frac{1}{n}\sum_{k=1}^{n}X_k-\mu\}\vert\lt\epsilon\}\ge1-\frac{\sigma^2/n}{\epsilon^2}$

当n趋向于无穷时: $\lim_{n\to\infty}{P(\vert\bar{X}_n-\mu\vert\lt\epsilon)}=1$.

可表示为: $\lim_{n\to\infty}P\{\bar{X}_n-\mu\ge\epsilon\}=0$

## 伯努利大数定理
设$f_A$是$n$次独立重复试验中事件A发生的次数$p$是事件A再每次实验中发生的概率，则对于任意整数$\epsilon\gt0$,有$\lim_{n\to\infty}P\{\vert\frac{f_A}{n}-p\vert\lt\epsilon\}=1$

## 辛钦大数定理和伯努利大数定理
辛钦大数定理说明了在n充分大的时候，算数平均依概率收敛于数学期望。伯努利大数定理说明了当试验次数足够大即n充分大时，事件发生的频率依概率收敛于事件发生的概率。

# 中心极限定理
> 中心极限定理指的是给定一个任意分布的总体，每次抽取n个样本，一共抽取m次，将n组抽样分别求平均值，这些平均值的分布接近于正态分布。若${X_n}$满足一定的条件，当$n$足够大时，$\bar{X_n}$近似服从正态分布，这就是中心极限定理的主要思想，也体现了正态分布的重要性与普遍性

不讲人话版本：客观实际中有许多随机变量，他们是由大量的相互独立的随机因素的综合影响所形成的。而其中每一个别因素在总的影响中所起的作用都是微小的。这种随机变量往往近似地服从正态分布。

## 独立同分布的中心极限定理
设随机变量$X_1,X_2,\dots,X_n,\dots$满足独立同分布，

具有数学期望$E(X)=\mu$和方差$D(X_k)=\sigma^2\gt0(k=1,2,\dots)$，

则随机变量之和$\sum_{k=1}^{n}X_k$的标准化变量:


$Y_n=\frac{\sum_{k=1}^nX_k-E(\sum_{k=1}{n}X_k)}{\sqrt{D(\sum_{k=1}^nX_k)}}=\frac{\sum_{k=1}^nX_k-n\mu}{\sqrt{n}\sigma}$满足标准正态分布。

## 李雅普诺夫(Lyapunov)定理
设随机变量$X_1,X_2,\dots，X_n,\dots$相互独立，他们具有数学期望和方差:

$E(X_k)=\mu_k, D(X_k)=\sigma^2_k\gt0, k=1,2,\dots$

记$B_n^2=\sum_{k=1}^n\sigma_k^2$

定理证明，在定理的条件下，随机变量$Z_n=\frac{\sum_{k=1}^nX_k-\sum_{k=1}^{n}\mu_k}{B_n}$当n很大时，近似地服从标准正态分布。

## 棣莫弗一拉普拉斯(De Moivre-Laplace)定理
 设随机变量$\eta_n(n=1,2,\dots)$服从参数为$n,p(0\lt p\lt1)$的二项分布，则对于任意$x$，有

 $\lim_{n\to\infty}P\{\frac{\eta_n-np}{\sqrt{np(1-p)}}\le x\}=\int_{-\infty}{x}\frac{1}{\sqrt(2\pi)}e^{-t^2/2}dt=\Phi(x)$

 ## 拓展链接
[李雅普诺夫定理的条件该怎么理解？- sola - 知乎](https://www.zhihu.com/question/68344634/answer/684052016)