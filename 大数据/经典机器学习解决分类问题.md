[toc]

# 分类器

## LogisticRegression with CountVectorizer

简单来说，  逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。 注意，这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。

### 逻辑回归与线性回归的关系

逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。 因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。

参考文献：

[逻辑回归（*Logistic Regression*](https://zhuanlan.zhihu.com/p/28408516)

## DummyClassifier

在开发机器学习模型时，最明智的做法是先创建一个基线模型。 本质上，该模型应该是“虚拟”模型，例如始终预测最频繁发生的类的模型。这里提供了一个基准来作为“智能”模型基准的基准，这样你就可以确保模型性能优于随机结果。

DummyClassfier是一种使用简单规则进行预测的分类器。通常该分类器作为一个简单的基线与其他(真实)分类器进行比较。

- most frequent: 预测值是出现频率最高的类别。

- stratified : 根据训练集中的频率分布给出随机预测。

- uniform: 使用等可能概率给出随机预测。
- constant: 根据用户的要求, 给出常数预测。

产生随机输出的虚拟分类器是所有分类器中最差的（精度最低），但也超过了百分之80。同时很难判断哪些结果是真正有帮助的（真正例）。

A dummy classifier is a type of classifier which does not generate any insight about the data and classifies the given data using only simple rules. The classifier’s behavior is completely independent of the training data as the trends in the training data are completely ignored and instead uses one of the strategies to predict the class label.

It is used only as a simple baseline for the other classifiers i.e. any other classifier is expected to perform better on the given dataset. It is especially useful for datasets where are sure of a class imbalance. It is based on the philosophy that any analytic approach for a classification problem should be better than a random guessing approach.

**Below are a few strategies used by the dummy classifier to predict a class label –**

1. **Most Frequent:** The classifier always predicts the most frequent class label in the training data.
2. **Stratified:** It generates predictions by respecting the class distribution of the training data. It is different from the “most frequent” strategy as it instead associates a probability with each data point of being the most frequent class label.
3. **Uniform:** It generates predictions uniformly at random.
4. **Constant:** The classifier always predicts a constant label and is primarily used when classifying non-majority class labels.

# model

TF-IDF和BoW见另一篇[TF-IDF与BoW](.\TF-IDF与BoW.md)

## fit_transform

fit_transform(): joins the fit() and transform() method for transformation of dataset.

解释：fit_transform是fit和transform的组合，既包括了训练又包含了转换。
transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）

fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。

## CountVectorizer

sklearn的CountVectorizer库是根据输入数据获取词频矩阵（稀疏矩阵）
fit(raw_documents) :根据CountVectorizer参数规则进行操作，比如滤除停用词等，拟合原始数据，生成文档中有价值的词汇表；

- transform(raw_documents):使用符合fit的词汇表或提供给构造函数的词汇表，从原始文本文档中提取词频，转换成词频矩阵。

  ![](https://img-blog.csdn.net/20181009094824519?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODI3ODMzNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- fit_transform(raw_documents, y=None):学习词汇词典并返回术语 - 文档矩阵(稀疏矩阵)。

  ![](https://img-blog.csdn.net/20181009094920533?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODI3ODMzNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

## TF-IDF Transformer

在较低的文本语料库中，一些词非常常见（例如，英文中的“the”，“a”，“is”），因此很少带有文档实际内容的有用信息。如果我们将单纯的计数数据直接喂给分类器，那些频繁出现的词会掩盖那些很少出现但是更有意义的词的频率。

为了重新计算特征的计数权重，以便转化为适合分类器使用的浮点值，通常都会进行TF-IDF转换。

- fit(raw_documents, y=None)：根据训练集生成词典和逆文档词频 由fit方法计算的每个特征的权重存储在model的idf_属性中。

  ![](https://img-blog.csdn.net/2018100909511248?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODI3ODMzNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

- transform(raw_documents, copy=True)：使用fit（或fit_transform）学习的词汇和文档频率（df），将文档转换为文档 - 词矩阵。返回稀疏矩阵，[n_samples, n_features]，即，Tf-idf加权文档矩阵（Tf-idf-weighted document-term matrix）。

  ![](https://img-blog.csdn.net/20181009095256259?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODI3ODMzNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

参考文献：

[fit_transform,fit,transform区别和作用详解](https://blog.csdn.net/weixin_38278334/article/details/82971752)

# 组合

## LogisticRegression with TF-IDF

使用TF-IDF加权文档矩阵，防止频繁出现的词掩盖很少出现但是更有意义的词的频率。

## LogisticRegression with n-gram TF-IDF

提取单词作为关键词，比较容易实现，如果要提取短语呢？

TF-IDF结合n-gram来提取关键短语，并根据单词长度、停用词和词性进行过滤。

N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。

每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。

该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。

在TfidfVectorizer中，ngram_range = (1, 3) 表示选取1到3个词做为组合方式: 词向量组合为: 'I', 'like', 'you', 'I like', 'like you', 'I like you' 构成词频标签

参考文献：

[关键词提取：TF-IDF和n-gram](https://blog.csdn.net/sjyttkl/article/details/105477593)

[自然语言处理中N-Gram模型介绍](https://zhuanlan.zhihu.com/p/32829048)
