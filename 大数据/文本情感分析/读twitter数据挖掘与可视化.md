参考文献:

[Twitter数据挖掘及其可视化](https://www.hrwhisper.me/twitter-data-mining-and-visualization/) 作者: hrwhisper

[LDA(理论篇)](https://blog.csdn.net/google19890102/article/details/50504870/) 作者:zhiyong_will

[文本建模](https://blog.csdn.net/jdbc/article/details/49661167) 

[TOC]

## LDA模型
2003年D.Blei等人提出LDA(潜在迪利克雷分布)主题模型，除了进行主题分析外，和可以用于文本分类和推荐系统等。

### 数学基本知识
#### 二项分布 

$P_n(k)=C^{k}_{n}p^k(1-p)^{n-k}$

#### 多项式分布:

二项式分布的推广形式，A取值可能有k种: $P(x_1,x_2,\dots,x_k;n;p_1,p_2,\dots,p_k)=\frac{n!}{x_1!x_2!\dots x_k!}p_1^{x_1}p_2^{x_2}\dots p_k^{x_k}$

#### Gamma分布 

$\Gamma(x)=\int_0^{\infty}e^{-u}u^{x-1}du$ 

- 性质1：$\Gamma(x+1)=x\Gamma(x)$ 

- 性质2：$\Gamma(1)=1,\Gamma(\frac{1}{2})=\sqrt{\pi}$ 

- 性质3：$\Gamma(n+1)=n!$

#### Beta分布:

k=2时的迪利克雷分布 

$Beta(a,b)=\int_0^1x^{a-1}(1-x)^{b-1}dx$ 对a>0,b>0$Beta(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$

#### 迪利克雷(Dirichlet)分布

基本形式:$D(a1,a2,\dots,a_k)=\int\dots\int x_1^{a_1-1}\dots x_k^{a_k-1}dx_1\dots dx_k$

#### 贝叶斯定理

$P(B∣A)=\frac{P(A∣B)P(B)}{P(A)}$

#### 共轭分布

> 先验分布+样本信息=>后验分布

上述的形式定义是贝叶斯派的思维方式，人们对于事物都会存在着最初的认识（先验分布），随着收集到越来越多的样本信息，新观察到的样本信息会不断修正人们对事物的最初的认识，最终得到对事物较为正确的认识（后验分布）。若这样的后验概率$P(θ∣x)$和先验概率$P(x)$满足同样的分布，那么先验分布和后验分布被称为共轭分布，同时，先验分布叫做似然函数的共轭先验分布。

如下两个性质:

- Beta分布是二项分布的共轭先验分布
- Dirichlet分布是多项式分布的共轭先验分布

### 文本建模

对于一篇文章，是文章中出现的次的过程，在文章中，我们已经知道每个词出现的概率，则在生成文章的过程中，我们在词库中根据概率取出每个词，形成一篇文章。

统计文本建模的目的就是追问这些观察到语料库中的的词序列是如何生成的。统计学被人们描述为猜测上帝的游戏，人类产生的所有的语料文本我们都可以看成是一个伟大的上帝在天堂中抛掷骰子生成的，我们观察到的只是上帝玩这个游戏的结果 —— 词序列构成的语料，而上帝玩这个游戏的过程对我们是个黑盒子。所以在统计文本建模中，我们希望猜测出上帝是如何玩这个游戏的，具体一点，最核心的两个问题是

- 上帝都有什么样的骰子；
- 上帝是如何抛掷这些骰子的；

第一个问题就是表示模型中都有哪些参数，骰子的每一个面的概率都对应于模型中的参数；第二个问题就表示游戏规则是什么，上帝可能有各种不同类型的骰子，上帝可以按照一定的规则抛掷这些骰子从而产生词序列。

#### Unigram Model

##### 频率派

- 词库中(不含停止词和介词)共V个词
- 每个词出现次数为:$n_1,n_2,\dots,n_v$,所有词出现的总次数为N
- 每个词对用的概率记为$\vec{p}=\{p_1,p_2,\dots,p_v\}$

我们假设文档与文档之间是相互独立的，而且进一步词与词之间也是相互独立的——词袋模型(Bag-of-words)。词袋模型认为名词的顺序是无关紧要的。

##### 贝叶斯派

对于贝叶斯派来说，其并不认同上述的求解参数值估计的方法，贝叶斯思维认为，一切的参数都是随机变量，因此上述的选择每个词的概率不是一个确定的值，而是一个随机变量，随机变量就应该服从一个分布。因此参数$\vec{p}$是由分布$P(\vec{p})$决定的，该分布称为先验分布。则上述的过程就变成了如下的两个过程：

- 首先由先验分布$P(\vec{p})$得到参数样本$\vec{p}$
- 由参数$\vec{p}$生成文档

#### 概率主题(Topic Model)模型

每一篇文章都会有一些主题，表示这篇文章主要讲的是关于哪方面的文章，而文章的基本组成单元式词，文章的主题则主要表现在词在不同组题的分布上，每一个词是在这些确定的主题上产生的，具体的如下图所示：

![主题模型](https://img-blog.csdn.net/20160114195043089)

文章的主题最终体现在词在每个主题的分布上。在写文章的过程中，首先我们需要做的是确定文章的主题，在确定了文章的主题的前提下，我们产生每一个词，从而构成了整篇文章。

如果要写一篇文章，我们往往是先确定其主题，比如这篇文章是写社会的，还是写的技术类的，或者游记类的，在主题确定的条件下，如要写一篇关于机器学习方面的文章，在确定了主题的条件下，会谈及到损失函数，模型，神经网络，深度学习等等，每个词在这篇文章中的比重会有所不同。这便是文章的生成过程，即：

> 一篇文章，通常是由多个主题构成的，而每个主题大概可以用于该主题相关的频率最高的一些词来描述。

### 通俗解释LDA

LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，用来推测文档的主题分布。它可以将文档集中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题分布后，便可以根据主题分布进行主题聚类或文本分类。

首先从主题库中随机抽取一个主题，编号为K，接着从词语库中不断取出主题为k的词，不断取出直到到达预计的文本长度为止。简单来讲就是随机选择某个主题，然后从该主题中随机选择词语。文档中词语生成概率为：$p(词语文档)=\sum_{主题}(p(词语主题)*p(主题文档))$

![LDA模型](https://www.hrwhisper.me/images/twitter-data-mining-and-visualization/LDA-matrix-multiply.png)

LDA模型的输入是一篇一篇用BOW(bag of words)表示的文档，用该文档中无序的单词序列表示该文档（忽略文档中的语法和词语的先后关系）。LDA的输出是每篇文章的主题分布矩阵和每个主题下的单词分布矩阵。简单来说，LDA模型的任务就是已知左边的矩阵，通过LDA采样方法(变分贝叶斯推断或吉布斯采样)得到右边两个矩阵。其中吉布斯采样也被称为蒙特卡洛马尔可夫(MCMC)采样方法。MCMC实现起来更加方便，VB速度比MCMC来的快，两者效果相同。对大量数据来讲，VB方法更好。

## 最具代表性的推文计算

运行模型后，得到了每个主题下对应的主题词，主题词对于主题的描述不够直观，为此我们希望从该主题下，找到最有代表性的推文。

#### KL-mean

KL散度(Kullback-Leibler divergence)又称为相对熵(relative entropy)，它可以衡量两个概率分布的相似程度。对于离散型随机变量，其概率分布P和Q的KL散度定义为：

$D_{KL}(PQ)=\sum_iP(i)ln\frac{P(i)}{Q(i)}$

通常来讲KL散度是非对称的，因此采用KL-mean的方式（求P和Q KL散度以及Q和P KL散度的均值）

$D_{KL-mean}(PQ)=\frac{1}{2}(D_{KL}(PQ)+D_{KL}(QP))$

使用KL-mean距离计算伪代码:

```
pro_matrix: 主题-单词矩阵
features = []
for tweet_id,tweet in tweets do
   topic_id根据文档-主题矩阵得到当前推文最大可能从属的主题序号
   feature = [0 …] // 长度为词库的大小的全0数组
   for word_id, each word in tweet do: //word_id 为当前单词在词库中的下标
            feature[word_id] = word_cnt * pro_matrix[topic_id][word_id] 
            //当前单词出现的次数乘以相应的主题-单词矩阵中的概率
        end for
   features.append(feature)
end for
对于所有相同主题序号的推文，计算其feature的平均值作为主题的中心。
接着使用KL-mean距离计算每条推文与其主题中心的距离dis
对于每个主题，找到与类中心最小距离的推文，该推文即为最具有代表性的推文。
```

#### 余弦距离

余弦距离常常用来很亮相似度。定义为：$D_{cos}(P,Q)=\frac{P\cdot Q}{|P|\cdot |Q|}$ 与KL散度方法过程类似,只不过最后采用余弦距离计算主题中心距离.

#### 最大熵

在信息学中,熵被用来衡量信息不确定度大小,信息的不确定度越大代表信息量越大.计算公式为

$Entropy(x)=-\sum_iP(x_i)log_2P(x_i)$

```
p: 主题-单词矩阵
entropy = []
for tweet_id,tweet in tweets do
   topic_id根据文档-主题矩阵得到当前推文最大可能从属的主题序号
   cur_entropy = 0
   for word_id, each word in tweet do: //word_id 为当前单词在词库中的下标
           cur_entropy += -p[topic_id][word_id] * log2 (p[topic_id][word_id])
        end for
   entropy.append(feature)
end for
对于每个主题，找到熵最大的推文，该推文即为最具有代表性的推文
```



## 情感分析
### 预处理

- POS标注
  - CMU ArkTweetNLP
- 字母连续三个相同
  - 替换"cooooool"=>"coool"
- 删除非英文单词
- 删除URL
- 删除@
  - 删除用户的提及@username
- 删除介词和停止词
- 否定展开
  - 将以"n't"结尾的单词进行拆封,如"don't"拆分为"do not",这里需要注意对一些词进行特殊处理,如"can't"拆分后结果为"can not",而不是"ca not"
- 否定处理
  - 从否定词(如shouldn't)开始到第一个标点之间的单词,均加入_NEG后缀.如perfect_NEG.

### 特征提取

- 文本特征
  - N-grams
    - 1~3元模型
    - 使用出现的次数而非频率表示.不仅是因为使用是否出现来表示特征有更好的效果,还因为推文本身比较短,一个短语不太可能在一篇推文中重复出现.
  - 感叹号问号个数
    - 句子中的感叹号和问号往往包含一定的情感.因此可以作为特征.
  - 的字母重复单词个数
    - 这是在与处理中对字母重复三次以上的单词进行计数.字母重复表达一定的情感.
  - 否定个数
    - 否定出现后,句子极性会发生反转.为此,把整个句子否定的个数作为一个特征.
  - 缩写词个数等
    - POS标注为['N','V','R','O','A']个数(名词,动词, 谓词,代词,形容词)
- 词典特征(本文使用感情词典有:Bing Linus词库,MPQA词库,NRC Hashtag词库和Sentiment140词库以及相应的经过否定处理的词库)
  - 推文中的单词在情感字典个数(即有极性的单词个数)
  - 推文总情感得分: 把每个存在于当前字典单词数相加，到推文的总情感得分：把每个存在于当前字典单词数相加，到推文的总情感得分：把每个存在于当前字典单词数相加，到推文总分，这个数作为一特征。
  - 推文中单词最大的正向情感得分和最小的负向情感得分.
  - 推文中所有正向情感的单词分数和以及所有负向情感单词的分数和.
  - 最后一个词的分数
- 表情特征
  - 推文中正向情感和负向情感的表情个数
  - 最后一个表情的极性是否为正向

### 特征选择

N-grams:

```
设定min_df（min_df>=0）以及threshold（0 <= threshold <= 1）
对于每个在N-grams的词:
统计其出现于正向、负向、中性的次数，得到pos_cnt, neg_cnt, neu_cnt，以及出现总数N,然后分别计算
pos = pos_cnt / N
neg = neg_cnt / N
neu = neu_cnt / N
对于 pos,neg,neu中任一一个大于阈值threshold 并且N > min_df的，保留该词，否则进行删除。
```

滤除了低频的词，因为这可能是一些拼写错误的词语；并且，删除了一些极性不那么明显的词，有效的降低了维度。

### 分类器选择

使用两个分类器进行对比，他们均使用sklearn提供的接口 。第一个分类器选用SVM线性核分类器，参数设置方面，C = 0.0021，其余均为默认值。第二个分类器是Logistic Regression分类器，其中，设置参数C=0.01105。

在特征选择上，min_df=5, threshold=0.6。

## 可视化

### 地理位置信息的可视化

- 对地理位置信息进行统计计数。一个可视化的办法就是在地图上根据经纬度坐标画一个个的点，但是当有多个点再一个小区域的时候可读性较差，因此本文使用的是热力图

### 话题结果可视化

- 矩形树图

  - 矩形树图是由一个个矩形递归组成的。

    同一个颜色表示同一主题，而矩形大小表示概率大小。

    在图形交互方面，矩形树图支持点击后放大查看。

- 气泡图

  - 同一个主题同一个圈，同一个圈内的圆大小表示概率的大小。

    在图形交互方面，气泡图支持点击后放大查看某一主题下的内容。

- 旭日图

  - 旭日图它可以说是饼状图的升级版。在最内圈的数据为每个主题，同时，用不同的颜色加以区分，内圈所占的大小就反映了主题的热度。接着，对于每个主题，向外延伸出对应的主题词，每个主题词占的面积大小就反映了其概率的大小。此外，本文做出了特殊的处理，将主题词中更重要的主题词在加一层显示。

    最重要的主题词计算方法为：按主题的概率从大到小排序，然后，从大到小进行遍历，对概率和进行累加，当对某一项i累加后的和大于0.4，则从第一个主题词到第i个主题词为该主题的最重要的主题词。

    旭日图的用户交互为，点击某一块区域，则图形变化为某主题下的单词概率分布饼图。

### 情感分析可视化

- 针对于情感分析，我们的任务是对于给定一些推文，判断其实情感类别。在分类结果完成后，我们可以对分类的结果进行统计。可以采用类似于对Hashtag的统计结果进行可视化的方法，如柱状图、饼状图，这里不再赘述。此外，还可以用“仪表盘”的方式来进行可视化。
