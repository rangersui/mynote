# Sentiment Analysis

[toc]

R.W Picard在《情感计算》一书中，她指出“情感计算就是针对人类的外在表现，能够进行测量和分析并能对情感施加影响的计算” <sup>[1]</sup> ，开辟了计算机科学的新领域，其思想是使计算机拥有情感，能够像人一样识别和表达情感。

## 文本情感分类

文本情感分类是文本分类的一个重要分支，也称为意见挖掘，按照处理文本的粒度不同，可分为词语短语级、句子级、篇章级等几个研究层次<sup>[2]</sup>。在情感分析中可以认为构成篇章的基本单位包括词、短语、和固定搭配，并且对于它们的褒贬程度的度量是判别文本情感倾向的基础。对文本情感分类的主要研究方法分为基于字典和基于语料库两种，在此基础上国内外学者进行了大量的研究。

#### 基于语料库

1997年，Riloff和Shepherd<sup>[3]</sup>在文本数据的基础上进行了构建语义词典的相关研究。同年，McKeown<sup>[4]</sup>发现连词对大规模的文本数据集中形容词的语义表达的制约作用，通过聚类算法对英文的形容词与连词做情感倾向研究。2003年，Turney和Littman<sup>[5]</sup>提出利用点互信息的方法扩展了正负面情感词典。

#### 基于字典

该类方法主要是根据词典WordNet或HowNet中词语间的关联来判别词语的极性。sista等<sup>[6]</sup>将General Inquirer和WordNet中的褒义和贬义词作为种子词，得到一个扩展后的较大规模情感词集合，并以此作为分类特征，利用机器学习方法对文本褒贬义进行了自动分类。Faye Baron和Graeme Hirst<sup>[7]</sup>从文档中抽取倾向性强的搭配作为种子词汇，取得了较好的分类效果。

#### 问题和挑战

文本情感挖掘的应用广泛，近年来取得了迅速的发展，但同时我们也该看到，文本情感分类是一个复杂的问题，在研究过程中还存在很多挑战。

##### 情感语义的机器理解问题

人类的自然语言情感表达十分复杂，特别是网络评论的形式更加灵活多变。要使机器精确的理解文本中的情感内容，不能简单的提取词语作为特征，还必须结合语言学方面的知识，借助于文本上下文和领域相关性对情感语义进行分析处理。

##### 特征提取问题

文本分类中一般采用BOW(bag of word，词袋法)表示文本的特征，然而由于情感表达中有许多使用诸如反语和隐喻等修辞手法的表达，并且上下文相关。例如“我今天吃了个苹果”和“苹果股价又跌了”根据文章的主题和上下文不同，两个“苹果”具有不同的意思。因此单独采用词袋法提取特征并进行分析的效果极其有限，如何提取对情感分析具有更大价值的特征依然是一个有挑战性的课题。

## 深度学习模型介绍

### 分类问题和回归问题

**“回归与分类的根本区别在于输出空间是否为一个度量空间。”**

我们不难看到，回归问题与分类问题本质上都是要建立映射关系：

> ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29%5Crightarrow+y%2Cx%5Cin+A%2C+y%5Cin+B)

而两者的区别则在于：

- 对于回归问题，其输出空间*B*是一个度量空间，即所谓“定量”。也就是说，回归问题的输出空间定义了一个度量 ![[公式]](https://www.zhihu.com/equation?tex=d%3DF%28y_%7Btrue%7D%2Cy_%7Bpred%7D%29) 去衡量输出值与真实值之间的“误差大小”。例如：预测一瓶700毫升的可乐的价格（真实价格为5元）为6元时，误差为1；预测其为7元时，误差为2。这两个预测结果是不一样的，是有度量定义来衡量这种“不一样”的。（于是有了均方误差这类误差函数）。
- 对于分类问题，其输出空间*B*不是度量空间，即所谓“定性”。也就是说，在分类问题中，只有分类“正确”与“错误”之分，至于错误时是将Class 5分到Class 6,还是Class 7，并没有区别，都是在error counter上+1。

参考文献：

[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/204957456)

### 神经网络组件

| 模型        | 功能                                             |
| ----------- | ------------------------------------------------ |
| LSTM/GRU    | 解决依赖问题，适合第一层建模，但比较慢           |
| CNN         | 配合池化抓取关键词特征                           |
| Capsules    | 代替CNN，效果更好                                |
| Max-pooling | 池化机制，只要关键特征                           |
| Attention   | 池化机制，突出关键特征，但是不能过滤掉非重点特征 |

对于短文本，CNN配合Max-pooling池化（如TextCNN模型）速度快，且效果好因为短文本关键词容易找到，Max-pooling会直接过滤掉模型认为不重要的特征。而Capsules效果比CNN更好，短文本上LSTM/GRU+Capusules模型更好，是短文本分类最好的baseline之一。

而长文本直接使用CNN效果并不好，在前面加一层LSTM效果能提升很多。

### TextCNN

![TextCNN](https://pic2.zhimg.com/80/v2-004f0d8c2becacf47779e778b2cfce8d_1440w.jpg)

基本工作机制为：卷积窗口沿着长度为n的文本一个个滑动，类似于n-gram机制对文本切词，然后和文本中的每个词进行相似度计算，因为后面接了个Max-pooling，因此只会保留和卷积核最相近的词。这就是TextCNN抓取关键词的机制。也可以组合LSTM和Capsules[胶囊网络](https://zhuanlan.zhihu.com/p/35409788)

### LSTM

#### RNN

<img src="https://pic4.zhimg.com/80/v2-f716c816d46792b867a6815c278f11cb_1440w.jpg" alt="naive RNN" style="zoom: 33%;" />

![[公式]](https://www.zhihu.com/equation?tex=x) 为当前状态下数据的输入， ![[公式]](https://www.zhihu.com/equation?tex=h) 表示接收到的上一个节点的输入。

![[公式]](https://www.zhihu.com/equation?tex=y) 为当前节点状态下的输出，而 ![[公式]](https://www.zhihu.com/equation?tex=h%27) 为传递到下一个节点的输出。

而 **y** 则常常使用 **h'** 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。

**y**如何通过 **h'** 计算得到往往看具体模型的使用方式。

<img src="https://pic2.zhimg.com/80/v2-71652d6a1eee9def631c18ea5e3c7605_1440w.jpg" alt="RNN" style="zoom:33%;" />

#### LSTM

LSTM是一种时间递归神经网络，它出现的原因是为了解决RNN的一个致命的缺陷。原生的RNN会遇到一个很大的问题，叫做The vanishing gradient problem for RNNs，也就是后面时间的节点会出现老年痴呆症，也就是忘事儿，这使得RNN在很长一段时间内都没有受到关注，网络只要一深就没法训练。之后研究人员开始使用递归神经网络来对时间关系进行建模，LSTM网络已被证明比传统的RNNS更加有效。

<img src="https://pic4.zhimg.com/80/v2-e4f9851cad426dfe4ab1c76209546827_1440w.jpg" alt="LSTM" style="zoom: 33%;" />

相比RNN只有一个传递状态 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et+) ，LSTM有两个传输状态，一个 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) （cell state），和一个 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) （hidden state）。（注：RNN中的 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 对于LSTM中的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) )

<img src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_1440w.jpg" alt="状态计算" style="zoom: 50%;" />

<img src="https://pic2.zhimg.com/v2-1da73eb309a56e65e515abf215eaa785_b.jpg" style="zoom:50%;" />

1.  忘记阶段。这个阶段主要是对上一个节点传进来的输入进行**选择性**忘记。简单来说就是会 “忘记不重要的，记住重要的”。具体来说是通过计算得到的 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ef) （f表示forget）来作为忘记门控，来控制上一个状态的 ![[公式]](https://www.zhihu.com/equation?tex=c%5E%7Bt-1%7D) 哪些需要留哪些需要忘。

   <img src="https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_1440w.jpg" alt="zf" style="zoom: 50%;" />

   <img src="https://pic4.zhimg.com/80/v2-15c5eb554f843ec492579c6d87e1497b_1440w.jpg" alt="zi" style="zoom: 25%;" />

2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 ![[公式]](https://www.zhihu.com/equation?tex=x%5Et) 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 ![[公式]](https://www.zhihu.com/equation?tex=z+) 表示。而选择的门控信号则是由 ![[公式]](https://www.zhihu.com/equation?tex=z%5Ei) （i代表information)来进行控制。

   > 将上面两步得到的结果相加，即可得到传输给下一个状态的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Et) 。也就是上图中的第一个公式。

3. 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 ![[公式]](https://www.zhihu.com/equation?tex=z%5Eo) 来进行控制的。并且还对上一阶段得到的 ![[公式]](https://www.zhihu.com/equation?tex=c%5Eo) 进行了放缩（通过一个tanh激活函数进行变化)。

与普通RNN类似，输出 ![[公式]](https://www.zhihu.com/equation?tex=y%5Et) 往往最终也是通过 ![[公式]](https://www.zhihu.com/equation?tex=h%5Et) 变化得到。

#### Keras中使用LSTM

input_shape = (None,1)中的None代表模型输入集的数据量不限制（即可以是过去任意天的工作量），1在这里代表只有工作量一个维度。

units = 50 代表将输入的维度映射成50个维度输出。return_sequences为True意味着返回多个单元短期的输出结果,为False则只返回一个单元的输出结果。

随后加入一层输出为1维的神经网络，并设置激活层函数为线性（选择线性的原因，是因为我这里试用了多种激活函数，发现线性激活函数效果最好）

最终，对模型进行编译，回归问题，我们选择损失函数为mse，优化器选择rmsprop。

```python
model = Sequential() #layers [1,50,50,50,50,1]

model.add(LSTM(input_shape=(None,1),units=50,return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(input_shape=(None,50),units=50,return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(input_shape=(None,50),units=50,return_sequences=True))
model.add(Dropout(0.2))

model.add(LSTM(50,return_sequences=False))
model.add(Dropout(0.2))

model.add(Dense(units=1))
model.add(Activation("linear"))
# model.add(Activation("sigmoid"))
```

batch_size是每次梯度更新的样本数，未写明则为32。值得说明一下的是batch_size这个参数的坑有点深，我看了好久才理解，它主要的作用其实是用较少的样本获得合适的优化模型的梯度方向，避免全样本迭代时过高的内存占用；  

epochs是指训练模型迭代次数，可以理解为我们遍历了一次所有的模型，则为1次epoch；  

validation_split代表将数据集中多少数据用作验证集的训练数据的比例，这里选择5%的结果进行验证。

```python
model.fit(X_train,y_train,batch_size=10,nb_epoch=100,validation_split=0.05)
```

参考文献:

[手把手教你构建一个LSTM时序网络](https://zhuanlan.zhihu.com/p/81606029)

#### Embedding

> 在深度学习的应用过程中，Embedding 这样一种将离散变量转变为连续向量的方式为神经网络在各方面的应用带来了极大的扩展。该技术目前主要有两种应用，NLP 中常用的 word embedding 以及用于类别数据的 entity embedding。

1. 作为监督性学习任务的输入。
2. 用于可视化不同离散变量之间的关系。

#### Embedding可视化

Embedding 最酷的一个地方在于它们可以用来可视化出表示的数据的相关性，当然要我们能够观察，需要通过降维技术来达到 2 维或 3 维。最流行的降维技术是：t-Distributed Stochastic Neighbor Embedding (TSNE)。

参考文献:

[Embedding的理解](https://zhuanlan.zhihu.com/p/46016518)

[Neural Network Embeddings Explained](https://link.zhihu.com/?target=https%3A//towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)

## 预处理

### 词形还原

如果我们提取的关键词是单个词，那么在使用TF-IDF进行提取之前，先要对每个单词做词形还原，不然salaries和salary会被认为是不同的两个单词。

词形还原是基于词典的，准确率比较高，所以使用NLTK做词形还原时，需要下载数据 WordNet。

## 没来得及看的文献

[情感分类方法简介](https://www.jianshu.com/p/61212b11769a)

## 参考文献

[1] Picard R W. Affective computing[M]. MIT press, 2000.

[2] Huang X J, Zhao J. Sentiment analysis for Chinese text[C]．Proceedings oftheCommunications of CCF,2008：4(2)．https://www.zhihu.com/question/267484520/answer/848248929)

[3] Riloff E, Shepherd J. A corpus-based approach for building semantic lexicons[J]. arXiv preprint cmp-lg/9706013, 1997.

[4] Hatzivassiloglou V，Mckeown K R．Predicting the semanticorientation of adjectives[C]．Proceedings of the 35thAnnual Meeting of the Association for Computational Linguistics and EighthConference of the European Chapter of the Association for ComputationalLinguistics．Association for Computational Linguistics，1997:174-181．

[5] Tumey P D，Littman M L．Measuring praise and criticism：Inference of semantic orientation from association[J].ACM Transactionson Information Systems，2003，21(4)：315-346．

[6] Sista S．P，Srinivasan S．H．Polarized lexicon for review classification[C．Proceedings of the International Conference on Artificial Intelligence．2004：867-872．

[7] Faye B，Graeme H．Collocations as cues to semanticorientation[C]．Proceedings of AAAI Spring Symposium onExploring Attitude and Affect in Text，2004．